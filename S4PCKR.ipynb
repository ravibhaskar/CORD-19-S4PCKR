{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Install Libraries and Models "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Install needed packages and NLP models\n!pip install -U pysolr\n!pip install -U scispacy\n!pip install -U jsonpath-ng\n!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_md-0.2.4.tar.gz\n!pip install -U pyvis\n    \n# !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_craft_md-0.2.4.tar.gz\n# !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_jnlpba_md-0.2.4.tar.gz\n# !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_bc5cdr_md-0.2.4.tar.gz\n# !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_bionlp13cg_md-0.2.4.tar.gz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget -O solr-8.5.0.zip \"https://archive.apache.org/dist/lucene/solr/8.5.0/solr-8.5.0.zip\";\n!unzip solr-8.5.0.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport os\nimport json\nimport glob\nimport re\nimport time\nimport pysolr\nimport csv\nimport time\nimport scipy\nimport spacy\nimport scispacy\nimport pandas as pd\nimport numpy as np\nimport networkx as nx\nimport ipywidgets as widgets\nfrom os import path\nfrom pandas import ExcelWriter\nfrom pandas import ExcelFile\nfrom jsonpath_ng.ext import parse\nfrom collections import Counter\nfrom collections import OrderedDict\nfrom IPython.core.display import display, HTML\nfrom pyvis.network import Network\nfrom datetime import date\nimport dateutil.parser as dparser\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocessing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text) :\n    text = re.sub(r\" ?\\([^)]*\\)\", \"\", text)\n    text = re.sub(r\" ?\\[[^)]*\\]\", \"\", text)\n    text = re.sub(r\"http\\S+\", \"\", text)\n    text = re.sub(r\"  \", \" \", text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_data() :\n    start = time.time()\n\n    meta_df = pd.read_csv('/kaggle/input/CORD-19-research-challenge/metadata.csv', sep=',', header=0)\n    meta_columns = list(meta_df.columns)\n    meta_df.fillna('', inplace=True)\n\n    # Filter to pick only needed sections\n    include_set = ['Abstract','Introduction', 'background', 'Discussion', 'Results', 'Results and Discussion', 'methods,results']\n    section_df = pd.read_excel('/kaggle/input/covidsectionheads/covid_section_heads.xlsx', sheet_name='Sheet1')\n    section_df.dropna(subset=['Section Heads'], inplace=True)\n    section_df = section_df[section_df['Section Heads'].isin(include_set)]\n\n    # Categories/Targets to pick from Section Heads column\n    introduction_categories = [\"Introduction\"]\n    discussion_categories = [\"Discussion\"]\n    result_categories = [\"Results\", \"Results and Discussion\", 'methods,results']\n\n    introduction_df = section_df[section_df['Section Heads'].isin(introduction_categories)]\n    discussion_df = section_df[section_df['Section Heads'].isin(discussion_categories)]\n    result_df = section_df[section_df['Section Heads'].isin(result_categories)]\n\n    intro_list = introduction_df.iloc[:, 0].tolist()\n    discussion_list = discussion_df.iloc[:, 0].tolist()\n    result_list = result_df.iloc[:, 0].tolist()\n\n    intro_list = list(map(lambda x: str(x).strip(), intro_list))\n    discussion_list = list(map(lambda x: str(x).strip(), discussion_list))\n    result_list = list(map(lambda x: str(x).strip(), result_list))\n\n    path = '/kaggle/input/CORD-19-research-challenge/'\n    paths = [p for p in glob.glob(path + \"**/*.json\", recursive=True)]\n    files_size = len(paths)\n\n    col_names = ['paper_id','title','source', 'abstract','introduction','discussion','result','body', 'has_covid']\n    clean_df = pd.DataFrame(columns=col_names)\n\n    covid_syns = ['COVID-19','COVID19','2019-nCoV','2019nCoV','Coronavirus','SARS-CoV-2','SARSCov2','novel Coronavirus']\n\n    target_empty_count = 0\n\n    abstract_expr = parse('$.abstract[*].text')\n\n    for path in paths:\n        with open(path) as f:\n            intro_text_list = list()\n            discussion_text_list = list()\n            result_text_list = list()\n\n            data = json.load(f)\n\n            abstract_texts = [match.value for match in abstract_expr.find(data)]\n\n            body_nodes = data['body_text']\n\n            for entry in body_nodes :\n                section_name = entry['section']\n                section_name = section_name.strip().lower()\n                entry_text = entry['text']\n\n                if section_name.strip() in intro_list:\n                    intro_text_list.append(entry_text)\n\n                if section_name.strip() in discussion_list:\n                    discussion_text_list.append(entry_text)\n\n                if section_name.strip() in result_list:\n                    result_text_list.append(entry_text)\n\n            if len(intro_text_list) == 0 and len(discussion_text_list) == 0 and len(result_text_list) == 0 :\n                target_empty_count = target_empty_count + 1\n\n\n            id = data['paper_id']\n            title = data['metadata']['title']\n\n            pubtime_df = meta_df[meta_df.sha == id]['publish_time']\n            pubtime_dict = pubtime_df.to_dict()\n            pubtime = ''\n            for pubtime_field_key in pubtime_dict.keys():\n                temp_pubtime_str = pubtime_dict.get(pubtime_field_key)\n                orig_temp_pubtime_str = temp_pubtime_str\n                try:\n                    temppubdate = dparser.parse(orig_temp_pubtime_str,fuzzy=True).date()\n                    pubtime = temppubdate.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n                except Exception as e:\n                    temp_pubtime_str_parts = temp_pubtime_str.split(' ')\n                    if len(temp_pubtime_str_parts) > 2 :\n                        try :\n                            temp_pubtime_str = temp_pubtime_str_parts[0] + ' ' + temp_pubtime_str_parts[1] + ' ' + temp_pubtime_str_parts[2]\n                            temppubdate = dparser.parse(temp_pubtime_str,fuzzy=True).date()\n                            pubtime = temppubdate.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n                        except Exception as ex:\n                            pubtime = ''\n                    else:\n                        pubtime = ''      \n\n            sha_df = meta_df[meta_df.sha == id]['source_x']\n            meta_dict = sha_df.to_dict()\n            source = ''\n            for meta_field_key in meta_dict.keys():\n                source = meta_dict.get(meta_field_key)\n\n            if not source:\n                title_df = meta_df[meta_df.title == title]['source_x']\n                meta_dict = title_df.to_dict()\n                for meta_field_key in meta_dict.keys():\n                    source = meta_dict.get(meta_field_key)\n\n            abstract = clean_text(\" \".join(abstract_texts))\n            introduction = clean_text(\" \".join(intro_text_list))\n            discussion = clean_text(\" \".join(discussion_text_list))\n            result = clean_text(\" \".join(result_text_list))\n            body = \" \".join([introduction, discussion, result])\n\n            has_covid = 'false'\n\n            res = [ele for ele in covid_syns if (ele.lower() in body.lower())]\n            if(len(res)  > 0):\n                has_covid = 'true'\n\n            if len(body.strip()) > 0 or len(abstract) > 0:\n                new_row = {'paper_id': id, 'title': title.strip(), 'source': source,'abstract': abstract.strip(),\n                           'introduction': introduction.strip(),'discussion': discussion.strip(),\n                           'result': result.strip(), 'body': body.strip(), 'publish_time': pubtime,'has_covid': has_covid}\n                clean_df = clean_df.append(new_row, ignore_index=True)\n\n    # Drop duoplicate papers\n    clean_df.drop_duplicates(subset=['title','abstract'], keep='first', inplace=False)\n    clean_df.to_csv('/kaggle/working/CORD-19.csv', index=True)\n\n    print('Final DataFrame Shape - ', clean_df.shape)\n    print(\"Papers that dont have Intro, Discussion or Result  - \", target_empty_count)\n    print('Total Papers processed - ', files_size)\n\n    print('Time Elaspsed - ', time.time() - start)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Configure Search Engine"},{"metadata":{"trusted":true},"cell_type":"code","source":"!solr-8.5.0/bin/solr start -force","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!solr-8.5.0/bin/solr create -c covid19 -s 1 -rf 1 -force","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using _default configset with data driven schema functionality. NOT RECOMMENDED for production use.\n!solr-8.5.0/bin/solr config -c covid19 -p 8983 -action set-user-property -property update.autoCreateFields -value false","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Set Up Synonyms\n\n!echo 'COVID-19,covid19,2019-nCoV,2019nCoV,Coronavirus,SARS-CoV-2,SARSCov2,novel Coronavirus' >> solr-8.5.0/server/solr/covid19/conf/synonyms.txt;\n!echo 'heart,cardiac,tachycardia,myocardial' >> solr-8.5.0/server/solr/covid19/conf/synonyms.txt;\n!echo 'pulmonary,respiratory' >> solr-8.5.0/server/solr/covid19/conf/synonyms.txt;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cat solr-8.5.0/server/solr/covid19/conf/synonyms.txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reload the covid19 core/collection because we added new synonyms. Need reload as it will affect index\n#Whenever new synonyms are added we need to reindex as synonyms are applied both on index and query analyzers\n!curl 'http://localhost:8983/solr/admin/cores?action=RELOAD&core=covid19'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Add custom field Type that wont tokenize phrases for fields like source etc\n!curl -X POST -H 'Content-type:application/json' --data-binary '{\"add-field-type\" : {\"name\":\"keywordText\",\"class\":\"solr.TextField\", \"positionIncrementGap\":\"100\", \"indexAnalyzer\" : {\"tokenizer\":{\"class\":\"solr.KeywordTokenizerFactory\" }, \"filters\":[{\"class\":\"solr.TrimFilterFactory\"},{\"class\":\"solr.StopFilterFactory\", \"ignoreCase\":true, \"words\":\"lang/stopwords_en.txt\"},{\"class\":\"solr.ManagedSynonymGraphFilterFactory\", \"managed\":\"english\" },{\"class\":\"solr.RemoveDuplicatesTokenFilterFactory\"},{\"class\":\"solr.FlattenGraphFilterFactory\"}]},\"queryAnalyzer\" : {\"tokenizer\":{\"class\":\"solr.KeywordTokenizerFactory\" },\"filters\":[{\"class\":\"solr.TrimFilterFactory\"},{\"class\":\"solr.StopFilterFactory\", \"ignoreCase\":true, \"words\":\"lang/stopwords_en.txt\"},{\"class\":\"solr.ManagedSynonymGraphFilterFactory\", \"managed\":\"english\" },{\"class\":\"solr.RemoveDuplicatesTokenFilterFactory\"}]}}}' http://localhost:8983/solr/covid19/schema","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create SOLR field definitions\n!curl -X POST -H 'Content-type:application/json' --data-binary '{\"add-field\": {\"name\":\"title\", \"type\":\"text_en_splitting_tight\", \"multiValued\":false, \"stored\":true, \"indexed\":true}}' http://localhost:8983/solr/covid19/schema;\n!curl -X POST -H 'Content-type:application/json' --data-binary '{\"add-field\": {\"name\":\"abstract\", \"type\":\"text_en_splitting_tight\", \"multiValued\":false, \"stored\":true, \"indexed\":true}}' http://localhost:8983/solr/covid19/schema;\n!curl -X POST -H 'Content-type:application/json' --data-binary '{\"add-field\": {\"name\":\"source\", \"type\":\"keywordText\", \"multiValued\":false, \"stored\":true, \"indexed\":true}}' http://localhost:8983/solr/covid19/schema;\n!curl -X POST -H 'Content-type:application/json' --data-binary '{\"add-field\": {\"name\":\"introduction\", \"type\":\"text_en_splitting_tight\", \"multiValued\":false, \"stored\":true, \"indexed\":true}}' http://localhost:8983/solr/covid19/schema;\n!curl -X POST -H 'Content-type:application/json' --data-binary '{\"add-field\": {\"name\":\"discussion\", \"type\":\"text_en_splitting_tight\", \"multiValued\":false, \"stored\":true, \"indexed\":true}}' http://localhost:8983/solr/covid19/schema;\n!curl -X POST -H 'Content-type:application/json' --data-binary '{\"add-field\": {\"name\":\"result\", \"type\":\"text_en_splitting_tight\", \"multiValued\":false, \"stored\":true, \"indexed\":true}}' http://localhost:8983/solr/covid19/schema;\n!curl -X POST -H 'Content-type:application/json' --data-binary '{\"add-field\": {\"name\":\"body\", \"type\":\"text_en_splitting_tight\", \"multiValued\":false, \"stored\":true, \"indexed\":true}}' http://localhost:8983/solr/covid19/schema;\n!curl -X POST -H 'Content-type:application/json' --data-binary '{\"add-field\": {\"name\":\"publish_time\", \"type\":\"pdate\", \"multiValued\":false, \"stored\":true, \"indexed\":true}}' http://localhost:8983/solr/covid19/schema;\n!curl -X POST -H 'Content-type:application/json' --data-binary '{\"add-field\": {\"name\":\"has_covid\", \"type\":\"boolean\", \"multiValued\":false, \"stored\":true, \"indexed\":true}}' http://localhost:8983/solr/covid19/schema;","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create Search Index"},{"metadata":{"trusted":true},"cell_type":"code","source":"solr = pysolr.Solr('http://localhost:8983/solr/covid19/', timeout=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generic_model = spacy.load('en_core_sci_md')\n\n#Load preprocessed CSV data\ncsv_path = '/kaggle/input/working/CORD-19.csv'\n\nif not path.exists(csv_path):\n    preprocess_data()\n    \ndf = pd.read_csv(csv_path, sep=',', header=0)\ndf.fillna('', inplace=True)\nprint('DF candidate_list size - ', df.shape)\n\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Index each pandas row as a document into SOLR search engine\n\ncovid_syns = ('SARSCoV2','SARS-CoV-2', '2019-nCoV','2019nCoV','COVID-19', 'COVID19','coronavirus', 'corona virus' 'novel coronavirus')\n\nlist_for_solr=[]\ncounter = 0\nfor index, row in df.iterrows():\n    id = row['paper_id']\n    title = row[\"title\"]\n    source = row[\"source\"]    \n    abstract = row[\"abstract\"]\n    introduction = row[\"introduction\"]\n    discussion = row[\"discussion\"]\n    result = row[\"result\"]\n    publish_time = row[\"publish_time\"]    \n    body = row[\"body\"]  # Cocatenated text of all text fields abstract, introduction, discussion, result\n    \n    if((title and title.isspace()) and (abstract and abstract.isspace()) and (body and body.isspace())):\n        continue\n        \n    has_covid = 'false'\n    if any(words in body for words in covid_syns):\n        has_covid = 'true'\n    \n    solr_content = {}\n    solr_content['id'] = id\n    solr_content['title'] = title\n    solr_content['source'] = source\n    solr_content['abstract'] = abstract\n    solr_content['introduction'] = introduction\n    solr_content['discussion'] = discussion\n    solr_content['result'] = result    \n    solr_content['body'] = body\n    solr_content['has_covid'] = has_covid\n    solr_content['publish_time'] = publish_time    \n        \n    list_for_solr.append(solr_content)\n    \n    if index % 1000 == 0:\n        solr.add(list_for_solr)\n        list_for_solr = []\n        counter = counter + 1000\n        print('Counter ', counter)\n        \n#Commit is very costly use it sparingly        \nsolr.commit()\nprint('Indexing Finished !')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_entities(models, text) :\n    entities = {}\n    \n    for nlp in models :\n        doc = nlp(text)\n        for ent in doc.ents:\n            entity = ent.text\n            if ent.label_ in entities :\n                if entities[ent.label_].count(ent.text) == 0:\n                    entities[ent.label_].append(ent.text)\n            else :\n                entities[ent.label_] = [ent.text]\n\n    return entities","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def initilize_nlp_models(model_names):\n    models = {}\n    for name in model_names:\n        models[name] = spacy.load(name)\n    \n    print('Models Loaded')\n    return models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def search_task_answers(search_results) :\n    answers_list = list()\n    \n    for search_result in search_results:\n        doc_hl_dict = {}\n        \n        id = search_result.get('id', \"MISSING\")\n        title = search_result.get('title', \"MISSING\")\n        \n        doc_highlights = search_results.highlighting[id]\n        \n        doc_hl_dict['id'] = id\n        doc_hl_dict['title'] = title\n        \n        if len(doc_highlights) > 0:\n            display(HTML(f'<h4><i>{title}\\n</i></h4>'))\n\n        for doc_hl_field in doc_highlights:\n            hl_snippets = doc_highlights[doc_hl_field]\n        \n            if len(hl_snippets) > 0 :\n                answer_snippet = ''\n                \n                #print('ID : ', id, '\\nTITLE : ', title)\n                #print('\\t', doc_hl_field)\n                display(HTML(f'\\t<h5>{doc_hl_field}\\n</h5>\\n'))\n            \n                for index, snippet in enumerate(hl_snippets, start=1):\n                    answer_snippet = answer_snippet.strip() + \" \" + snippet.strip()\n                    \n                    display(HTML(f'<blockquote>{index}. {snippet.strip()}\\n</blockquote>'))\n                    #print('\\t\\t', index , '. ' , snippet.strip(), '\\n') \n                                  \n                doc_hl_dict[doc_hl_field] = answer_snippet.strip()\n                    \n        if len(doc_hl_dict) > 0:\n            answers_list.append(doc_hl_dict)\n        \n    return answers_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def search(query, rows=5):\n    # Search for data\n    search_results = solr.search(query, rows, **{\n    'fq':'has_covid:true',\n    'qf':'title^50.0 abstract^40.0 introduction^30.0 discussion^20.0 result^50.0 body^10.0',\n    'pf':'title^60.0 abstract^50.0 introduction^40.0 discussion^30.0 result^60.0 body^20.0',\n    'hl': 'true',\n    'hl.bs.type': 'SENTENCE',\n    'hl.method' : 'unified',\n    'hl.snippets' : 5,\n    'hl.usePhraseHighlighter': 'true',\n    'hl.highlightMultiTerm' : 'true',\n    'hl.tag.pre':'',\n    'hl.tag.post':'',\n    'df':'body',\n    'hl.fl':'introduction,discussion,result'\n    })\n\n    num_docs_found = search_results.hits\n    num_search_results = len(search_results)\n    display(HTML(f'<h3 style=\"color:blue\">Top {num_search_results} search result(s) of {num_docs_found} total. \\n</h3>'))\n    \n    return num_docs_found, search_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def populate_labels(task_answers) :\n    field_list = ['introduction','discussion', 'result']\n\n    for doc_answer_dict in task_answers:\n        all_entities = set()\n        intro_entities = set()\n        discussion_entities = set()\n        result_entities = set() \n\n        for field_name, answer_text in doc_answer_dict.items():\n            if field_name in field_list:\n                ent_dict = extract_entities(chosen_models, answer_text)\n                ent_list = ent_dict['ENTITY']\n\n                all_entities.update(ent_list)\n                if field_name == 'introduction' :\n                    intro_entities.update(ent_list)\n                elif field_name == 'discussion' :\n                    discussion_entities.update(ent_list)\n                else :\n                    result_entities.update(ent_list)\n\n        # Now set up labels for entities\n        prior_newdata_entities = set()\n        prior_strong_entities = set()\n        prior_entities = set()\n        speculative_entities = set()\n        unknown_entities = set()    \n        novel_entities = set()\n\n        for a_ent in all_entities :\n            if a_ent in intro_entities and a_ent in result_entities and a_ent in discussion_entities : \n                prior_newdata_entities.add(a_ent)\n            elif a_ent in intro_entities and a_ent not in result_entities and a_ent in discussion_entities :\n                prior_strong_entities.add(a_ent)\n            elif a_ent in intro_entities and a_ent not in result_entities and a_ent not in discussion_entities :\n                prior_entities.add(a_ent)\n            elif a_ent not in intro_entities and a_ent in result_entities and a_ent not in discussion_entities : \n                unknown_entities.add(a_ent)\n            elif a_ent not in intro_entities and a_ent in result_entities and a_ent in discussion_entities :\n                novel_entities.add(a_ent)\n            else :\n                pass\n            \n        if(len(prior_newdata_entities) > 0) :\n            doc_answer_dict['prior-newdata'] = list(prior_newdata_entities)\n\n        if(len(prior_strong_entities) > 0) :\n            doc_answer_dict['prior-strong'] = list(prior_strong_entities)          \n\n        if(len(prior_entities) > 0) :\n            doc_answer_dict['prior'] = list(prior_entities)             \n\n        if(len(speculative_entities) > 0) :\n            doc_answer_dict['speculative'] = list(speculative_entities)\n\n        if(len(unknown_entities) > 0) :\n            doc_answer_dict['unknown'] = list(unknown_entities)           \n\n        if(len(novel_entities) > 0) :\n            doc_answer_dict['novel'] = list(novel_entities)           \n\n    return task_answers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### What do we know about COVID-19 risk factors?\n\nTask Details\n\nWhat do we know about COVID-19 risk factors? What have we learned from epidemiological studies?\nSpecifically, we want to know what the literature reports about:\n\n* Data on potential risks factors\n    1. Smoking, pre-existing pulmonary disease\n    2. Co-infections (determine whether co-existing respiratory/viral infections make the virus more transmissible or virulent) and other co-morbidities\n    3. Neonates and pregnant women\n    4. Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.\n* Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors\n* Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups\n* Susceptibility of populations\n* Public health mitigation measures that could be effective for control\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"tasks = ['Smoking and pre-existing pulmonary disease', \n         'Co-infections, co-morbidities and respiratory infections',\n         'Neonates and pregnant women',\n         'Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.',\n         'Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors',\n         'Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups',\n         'Susceptibility of populations',\n         'Public health mitigation measures that could be effective for control'\n        ]\n\nqueries = ['(\"Smoking COVID-19\"~10 OR \"pulmonary disease\"~10)', \n           '(\"Co-infections COVID-19\"~10 OR \"co-morbidities COVID-19\"~10 OR \"respiratory infections COVID-19\"~10)',\n           '(\"Neonates COVID-19\"~10 OR \"pregnant women COVID-19\"~10)',\n           'Socio-economic, behavioral factors and economic impact',\n           'Transmission dynamics, basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors',\n           'Severity of disease, risk of fatality among symptomatic hospitalized patients and high-risk patient groups',\n           'Susceptibility of populations',\n           'Public health mitigation measures that are effective for control'\n        ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Load NLP Models only once\n# model_names = ['en_ner_craft_md','en_ner_jnlpba_md', 'en_ner_bc5cdr_md','en_ner_bionlp13cg_md']\n# models_dict = initilize_nlp_models(model_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Map entities existence in intro-result-discussion respectively to label values\nlabel_def = {'111':'prior-newdata','101':'prior-strong','100':'prior','001':'speculative','010':'unknown', '011':'novel'}\nchosen_models = list()\nchosen_models.append(generic_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_def_list = list(label_def.values())\n\nfor task,query in zip(tasks, queries):\n    label_entities = []\n    display(HTML(f'<h3 style=\"color:red\">Task - {task} \\n</h3>'))\n    numDocsFound, search_results = search(query)\n    if numDocsFound > 0:\n        task_answers = search_task_answers(search_results)\n        task_answers = populate_labels(task_answers)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_names = ['en_ner_craft_md','en_ner_jnlpba_md','en_ner_bc5cdr_md','en_ner_bionlp13cg_md']\nmodels = initilize_nlp_models(model_names)\n\nq_entity_list = []\nfor task,query in zip(tasks, queries):\n    label_entities = []\n    num_docs_found, search_results = search(q_text, 100)\n    for search_result in search_results:\n        body = search_result.get('body', \"\")\n        for model in models.values(): \n            body_doc = model(body)        \n            q_entity_list = q_entity_list + [e.text for e in body_doc.ents]\n\n    nerCntr = Counter(q_entity_list)\n    freq_ners = nerCntr.most_common(50)\n    print(freq_ners)\n    print(len(freq_ners))\n\n    x,y = zip(*freq_ners)\n    x,y = list(x),list(y)\n\n    plt.figure(figsize=(15,10))\n    ax= sns.barplot(x=x, y=y,palette = sns.cubehelix_palette(len(x)))\n    plt.xlabel('Entity')\n    plt.xticks(rotation=90)\n    plt.ylabel('Frequency')\n    plt.title(task)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def search_custom_query(query):\n    numDocsFound, search_results = search(query)\n    #display(search_results)\n    if numDocsFound > 0:\n        task_answers = search_task_answers(search_results)\n        task_answers = populate_labels(task_answers)\n        #display(task_answers)\n        \n        return task_answers\n\nsearchbar = widgets.interactive(search_custom_query, query='Pregnant women')\nsearchbar","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualization of Force Directed Graph codes"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_query = searchbar.kwargs\nsearched_query = temp_query['query']\na = search_custom_query(searched_query)\ntask_answers = a","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cosine_score(x):\n        d = []\n        for i in range(len(x)):\n            for j in range(i+1,len(x)):\n                doc1= generic_model(x[i])\n                doc2= generic_model(x[j])\n                d.append({\n\n                    'Title1': x[i],\n                    'Title2': x[j],\n                    'Score': doc1.similarity(doc2)*30\n                }\n            )\n\n        return d\n\nclass network_graph:\n    \n    def __init__(self,search_result):\n        data = pd.DataFrame(search_result)\n        self.data = data\n        col_list = data.columns\n        drop_list = ['introduction','discussion','result']\n        self.new_cols = []\n        for cols in col_list:\n            if cols not in drop_list:\n                self.new_cols.append(cols)\n\n        \n    def extract_titles(self,search_result):\n        title_list = []\n        for title in range(len(search_result)):\n            title_list.append(search_result[title]['title'])\n            \n        title_dict_temp = cosine_score(title_list)\n        self.title_df = pd.DataFrame(title_dict_temp)\n        title_df_temp1 = self.title_df[['Title1']]\n        title_df_temp1.rename(columns={'Title1':'Title'},inplace=True)\n        title_df_temp2 = self.title_df[['Title2']]\n        title_df_temp2.rename(columns={'Title2':'Title'},inplace=True)\n        self.merged_titles = pd.concat([title_df_temp1,title_df_temp2],axis=0)\n        \n    \n    def extract_words(self,search_result):\n        \n        word_df = self.data[self.new_cols]\n        \n        word_df_temp = word_df.drop(['id','title'],axis=1)\n        \n        col_list = list(word_df_temp.columns)\n        \n        self.word_df_prior =pd.DataFrame()\n        self.word_df_prior_strong =pd.DataFrame()\n        self.word_df_prior_newdata =pd.DataFrame()\n        self.word_df_speculative =pd.DataFrame()\n        self.word_df_unknown =pd.DataFrame()\n        self.word_df_novel =pd.DataFrame()\n        \n        word_df_prior_1 =pd.DataFrame()\n        word_df_prior_strong_1 =pd.DataFrame()\n        word_df_prior_newdata_1 =pd.DataFrame()\n        word_df_speculative_1 =pd.DataFrame()\n        word_df_unknown_1 =pd.DataFrame()\n        word_df_novel_1 =pd.DataFrame()\n            \n        for col in col_list:\n            if col =='prior':\n                self.word_df_prior = word_df[['id','title','prior']]\n                self.word_df_prior = self.word_df_prior.explode('prior')\n                self.word_df_prior.dropna(subset = [\"prior\"], inplace=True)\n                self.word_df_prior['Weight'] = 8\n                \n                word_df_prior_1 = self.word_df_prior[['prior']]\n                word_df_prior_1.drop_duplicates(inplace=True)\n                word_df_prior_1['Color'] = 'tomato'\n                word_df_prior_1['Size'] = 10\n                word_df_prior_1.columns = ['Words','Color','Size']\n                \n            elif col== 'prior-strong':\n                self.word_df_prior_strong = word_df[['id','title','prior-strong']]\n                self.word_df_prior_strong = self.word_df_prior_strong.explode('prior-strong')\n                self.word_df_prior_strong.dropna(subset = [\"prior-strong\"], inplace=True)\n                self.word_df_prior_strong['Weight'] = 6\n                \n                word_df_prior_strong_1 = self.word_df_prior_strong[['prior-strong']]\n                word_df_prior_strong_1.drop_duplicates(inplace=True)\n                word_df_prior_strong_1['Color'] = 'sienna'\n                word_df_prior_strong_1['Size'] = 10\n                word_df_prior_strong_1.columns = ['Words','Color','Size']\n                \n                \n            elif col == 'prior_newdata':\n                self.word_df_prior_newdata = word_df[['id','title','prior_newdata']]\n                self.word_df_prior_newdata = self.word_df_prior_newdata.explode('prior_newdata')\n                self.word_df_prior_newdata.dropna(subset = [\"prior_newdata\"], inplace=True)\n                self.word_df_prior_newdata['Weight'] = 4\n                \n                word_df_prior_newdata_1 = self.word_df_prior_newdata[['prior_newdata']]\n                word_df_prior_newdata_1.drop_duplicates(inplace=True)\n                word_df_prior_newdata_1['Color'] = 'bisque'\n                word_df_prior_newdata_1['Size'] = 10\n                word_df_prior_newdata_1.columns = ['Words','Color','Size']\n                \n                \n            elif col =='speculative':\n                self.word_df_speculative = word_df[['id','title','speculative']]\n                self.word_df_speculative = self.word_df_speculative.explode('speculative')\n                word_df_speculative.dropna(subset = [\"speculative\"], inplace=True)\n                self.word_df_speculative['Weight'] = 10\n                \n                word_df_speculative_1 = self.word_df_speculative[['speculative']]\n                word_df_speculative_1.drop_duplicates(inplace=True)\n                word_df_speculative_1['Color'] = 'dimgray'\n                word_df_speculative_1['Size'] = 10\n                word_df_speculative_1.columns = ['Words','Color','Size']\n                \n            elif col =='unknown':\n                self.word_df_unknown = word_df[['id','title','unknown']]\n                self.word_df_unknown = self.word_df_unknown.explode('unknown')\n                self.word_df_unknown.dropna(subset = [\"unknown\"], inplace=True)\n                self.word_df_unknown['Weight'] = 30\n                \n                word_df_unknown_1 = self.word_df_unknown[['unknown']]\n                word_df_unknown_1.drop_duplicates(inplace=True)\n                word_df_unknown_1['Color'] = 'black'\n                word_df_unknown_1['Size'] = 10\n                word_df_unknown_1.columns = ['Words','Color','Size']\n                \n            elif col =='novel':\n                self.word_df_novel = word_df[['id','title','novel']]\n                self.word_df_novel = self.word_df_novel.explode('novel')\n                self.word_df_novel.dropna(subset = [\"novel\"], inplace=True)\n                self.word_df_novel['Weight'] = 2\n                \n                word_df_novel_1 = self.word_df_novel[['novel']]\n                word_df_novel_1.drop_duplicates(inplace=True)\n                word_df_novel_1['Color'] = 'purple'\n                word_df_novel_1['Size'] = 10\n                word_df_novel_1.columns = ['Words','Color','Size']\n                \n    # Take all the titles and categorize them into one group\n    \n        title_df_1 = self.merged_titles[['Title']]\n        title_df_1.drop_duplicates(inplace=True,keep='first')\n        title_df_1['Color'] = 'firebrick'\n        title_df_1['Size'] = 30\n        title_df_1.columns = ['Words','Color','Size']\n        \n        # Combining all the dataframe together in one place\n\n        frames = [title_df_1,word_df_prior_1,word_df_prior_strong_1,word_df_prior_newdata_1,word_df_speculative_1,word_df_unknown_1,word_df_novel_1]\n         \n        self.merged_df = pd.concat(frames)\n        \n        \n    # Changing the index of merged_df to 'Words' so that can combine it with node\n\n        self.merged_df_index = self.merged_df.drop_duplicates(subset='Words',keep='first')\n        self.merged_df_index.set_index('Words',inplace=True)\n        \n        \n    def network_creation(self):\n        \n        i = nx.Graph()\n        \n        \n\n        if (self.title_df.empty ==False):\n            for row in self.title_df.iterrows():\n                i.add_edge(row[1]['Title1'], row[1]['Title1'], weight=row[1]['Score'])\n\n        if (self.word_df_prior.empty ==False):\n            for row in self.word_df_prior.iterrows():\n                i.add_edge(row[1]['title'], row[1]['prior'], weight=row[1]['Weight'])\n\n        if (self.word_df_prior_strong.empty ==False):\n            for row in self.word_df_prior_strong.iterrows():\n                i.add_edge(row[1]['title'], row[1]['prior-strong'], weight=row[1]['Weight'])\n            \n        if (self.word_df_novel.empty ==False):\n            for row in self.word_df_novel.iterrows():\n                i.add_edge(row[1]['title'], row[1]['novel'], weight=row[1]['Weight'])\n                \n        if (self.word_df_speculative.empty ==False):\n            for row in self.word_df_speculative.iterrows():\n                i.add_edge(row[1]['title'], row[1]['speculative'], weight=row[1]['Weight'])\n                \n        if (self.word_df_prior_newdata.empty ==False):\n            for row in self.word_df_prior_newdata.iterrows():\n                i.add_edge(row[1]['title'], row[1]['prior_newdata'], weight=row[1]['Weight'])\n                \n        if (self.word_df_unknown.empty ==False):\n            for row in self.word_df_unknown.iterrows():\n                i.add_edge(row[1]['title'], row[1]['unknown'], weight=row[1]['Weight'])\n            \n        merged_df_clr = self.merged_df_index.reindex(i.nodes())\n\n        merged_df_clr['Color']=pd.Categorical(merged_df_clr['Color'])\n\n        merged_df_clr['Color'].cat.codes\n        \n        \n        #Plotting the force directed graph\n        \n        plt.figure(figsize=(22, 22))\n        degrees = nx.degree(i)\n        pos_node = nx.spring_layout(i,k=0.5)\n        nx.draw_networkx(i,pos=pos_node,node_color=merged_df_clr['Color'].cat.codes, cmap=plt.cm.Set2,node_size=[(degrees[v] + 1) * 100 for v in i.nodes()],alpha = 0.7)\n        \n    def force_directed_graphs(self):\n        \n        net = Network(notebook=True)\n\n        temp1 = self.merged_df\n        temp2 = temp1.reset_index()\n\n        temp2.drop(columns=['index'],inplace=True)\n\n        net.add_nodes(temp2['Words'],title = temp2['Words'],color=temp2['Color'],size=temp2['Size'].to_list())\n\n        if (self.title_df.empty ==False):\n            for row in self.title_df.iterrows():\n                net.add_edge(row[1]['Title1'], row[1]['Title1'], weight=row[1]['Score'])\n\n        if (self.word_df_prior.empty ==False):\n            for row in self.word_df_prior.iterrows():\n                net.add_edge(row[1]['title'], row[1]['prior'], weight=row[1]['Weight'])\n\n        if (self.word_df_prior_strong.empty ==False):\n            for row in self.word_df_prior_strong.iterrows():\n                net.add_edge(row[1]['title'], row[1]['prior-strong'], weight=row[1]['Weight'])\n            \n        if (self.word_df_novel.empty ==False):\n            for row in self.word_df_novel.iterrows():\n                net.add_edge(row[1]['title'], row[1]['novel'], weight=row[1]['Weight'])\n                \n        if (self.word_df_speculative.empty ==False):\n            for row in self.word_df_speculative.iterrows():\n                net.add_edge(row[1]['title'], row[1]['speculative'], weight=row[1]['Weight'])\n                \n        if (self.word_df_prior_newdata.empty ==False):\n            for row in self.word_df_prior_newdata.iterrows():\n                net.add_edge(row[1]['title'], row[1]['prior_newdata'], weight=row[1]['Weight'])\n                \n        if (self.word_df_unknown.empty ==False):\n            for row in self.word_df_unknown.iterrows():\n                net.add_edge(row[1]['title'], row[1]['unknown'], weight=row[1]['Weight'])\n\n\n        # net.enable_physics(True)\n        display(net.show(\"mygraph.html\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calling all the functions\nng= network_graph(task_answers)\nng.extract_titles(task_answers)\nng.extract_words(task_answers)\nng.force_directed_graphs()\n# ng.network_creation()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}