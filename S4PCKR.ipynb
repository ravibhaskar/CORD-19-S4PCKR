{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Install Libraries and Models "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Install needed packages and NLP models\n!pip install -U pysolr\n!pip install -U scispacy\n!pip install -U jsonpath-ng\n!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_md-0.2.4.tar.gz\n    \n# !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_craft_md-0.2.4.tar.gz\n# !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_jnlpba_md-0.2.4.tar.gz\n# !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_bc5cdr_md-0.2.4.tar.gz\n# !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_bionlp13cg_md-0.2.4.tar.gz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget -O solr-8.5.0.zip \"https://archive.apache.org/dist/lucene/solr/8.5.0/solr-8.5.0.zip\";\n!unzip solr-8.5.0.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport os\nimport json\nimport glob\nimport re\nimport time\nimport pysolr\nimport csv\nimport time\nimport scipy\nimport spacy\nimport scispacy\nimport pandas as pd\nimport numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport ipywidgets as widgets\nfrom os import path\nfrom pandas import ExcelWriter\nfrom pandas import ExcelFile\nfrom jsonpath_ng.ext import parse\nfrom collections import Counter\nfrom collections import OrderedDict\nfrom IPython.core.display import display, HTML\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocessing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text) :\n    text = re.sub(r\" ?\\([^)]*\\)\", \"\", text)\n    text = re.sub(r\" ?\\[[^)]*\\]\", \"\", text)\n    text = re.sub(r\"http\\S+\", \"\", text)\n    text = re.sub(r\"  \", \" \", text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_data() :\n    start = time.time()\n\n    meta_df = pd.read_csv('/kaggle/input/CORD-19-research-challenge/metadata.csv', sep=',', header=0)\n    meta_columns = list(meta_df.columns)\n    meta_df.fillna('', inplace=True)\n\n    # Filter to pick only needed sections\n    include_set = ['Abstract','Introduction', 'background', 'Discussion', 'Results', 'Results and Discussion', 'methods,results']\n    section_df = pd.read_excel('/kaggle/input/covidsectionheads/covid_section_heads.xlsx', sheet_name='Sheet1')\n    section_df.dropna(subset=['Section Heads'], inplace=True)\n    section_df = section_df[section_df['Section Heads'].isin(include_set)]\n\n    # Categories/Targets to pick from Section Heads column\n    introduction_categories = [\"Introduction\"]\n    discussion_categories = [\"Discussion\"]\n    result_categories = [\"Results\", \"Results and Discussion\", 'methods,results']\n\n    introduction_df = section_df[section_df['Section Heads'].isin(introduction_categories)]\n    discussion_df = section_df[section_df['Section Heads'].isin(discussion_categories)]\n    result_df = section_df[section_df['Section Heads'].isin(result_categories)]\n\n    intro_list = introduction_df.iloc[:, 0].tolist()\n    discussion_list = discussion_df.iloc[:, 0].tolist()\n    result_list = result_df.iloc[:, 0].tolist()\n\n    intro_list = list(map(lambda x: str(x).strip(), intro_list))\n    discussion_list = list(map(lambda x: str(x).strip(), discussion_list))\n    result_list = list(map(lambda x: str(x).strip(), result_list))\n\n    path = '/kaggle/input/CORD-19-research-challenge/'\n    paths = [p for p in glob.glob(path + \"**/*.json\", recursive=True)]\n    files_size = len(paths)\n\n    col_names = ['paper_id','title','source', 'abstract','introduction','discussion','result','body', 'has_covid']\n    clean_df = pd.DataFrame(columns=col_names)\n\n    covid_syns = ['COVID-19','COVID19','2019-nCoV','2019nCoV','Coronavirus','SARS-CoV-2','SARSCov2','novel Coronavirus']\n\n    target_empty_count = 0\n\n    abstract_expr = parse('$.abstract[*].text')\n\n    for path in paths:\n        with open(path) as f:\n            intro_text_list = list()\n            discussion_text_list = list()\n            result_text_list = list()\n\n            data = json.load(f)\n\n            abstract_texts = [match.value for match in abstract_expr.find(data)]\n\n            body_nodes = data['body_text']\n\n            for entry in body_nodes :\n                section_name = entry['section']\n                section_name = section_name.strip().lower()\n                entry_text = entry['text']\n\n                if section_name.strip() in intro_list:\n                    intro_text_list.append(entry_text)\n\n                if section_name.strip() in discussion_list:\n                    discussion_text_list.append(entry_text)\n\n                if section_name.strip() in result_list:\n                    result_text_list.append(entry_text)\n\n            if len(intro_text_list) == 0 and len(discussion_text_list) == 0 and len(result_text_list) == 0 :\n                target_empty_count = target_empty_count + 1\n\n\n            id = data['paper_id']\n            title = data['metadata']['title']\n\n            pubtime_df = meta_df[meta_df.sha == id]['publish_time']\n            pubtime_dict = pubtime_df.to_dict()\n            pubtime = ''\n            for pubtime_field_key in pubtime_dict.keys():\n                pubtime = pubtime_dict.get(pubtime_field_key)        \n\n            sha_df = meta_df[meta_df.sha == id]['source_x']\n            meta_dict = sha_df.to_dict()\n            source = ''\n            for meta_field_key in meta_dict.keys():\n                source = meta_dict.get(meta_field_key)\n\n            if not source:\n                title_df = meta_df[meta_df.title == title]['source_x']\n                meta_dict = title_df.to_dict()\n                for meta_field_key in meta_dict.keys():\n                    source = meta_dict.get(meta_field_key)\n\n            abstract = clean_text(\" \".join(abstract_texts))\n            introduction = clean_text(\" \".join(intro_text_list))\n            discussion = clean_text(\" \".join(discussion_text_list))\n            result = clean_text(\" \".join(result_text_list))\n            body = \" \".join([introduction, discussion, result])\n\n            has_covid = 'false'\n\n            res = [ele for ele in covid_syns if (ele.lower() in body.lower())]\n            if(len(res)  > 0):\n                has_covid = 'true'\n\n            if len(body.strip()) > 0 or len(abstract) > 0:\n                new_row = {'paper_id': id, 'title': title.strip(), 'source': source,'abstract': abstract.strip(),\n                           'introduction': introduction.strip(),'discussion': discussion.strip(),\n                           'result': result.strip(), 'body': body.strip(), 'publish_time': pubtime,'has_covid': has_covid}\n                clean_df = clean_df.append(new_row, ignore_index=True)\n\n    # Drop duoplicate papers\n    clean_df.drop_duplicates(subset=['title','abstract'], keep='first', inplace=False)\n    clean_df.to_csv('/kaggle/working/CORD-19.csv', index=True)\n\n    print('Final DataFrame Shape - ', clean_df.shape)\n    print(\"Papers that dont have Intro, Discussion or Result  - \", target_empty_count)\n    print('Total Papers processed - ', files_size)\n\n    print('Time Elaspsed - ', time.time() - start)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Configure Search Engine"},{"metadata":{"trusted":true},"cell_type":"code","source":"!solr-8.5.0/bin/solr start -force","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!solr-8.5.0/bin/solr create -c covid19 -s 1 -rf 1 -force","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using _default configset with data driven schema functionality. NOT RECOMMENDED for production use.\n!solr-8.5.0/bin/solr config -c covid19 -p 8983 -action set-user-property -property update.autoCreateFields -value false","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Set Up Synonyms\n\n!echo 'COVID-19,covid19,2019-nCoV,2019nCoV,Coronavirus,SARS-CoV-2,SARSCov2,novel Coronavirus' >> solr-8.5.0/server/solr/covid19/conf/synonyms.txt;\n!echo 'heart,cardiac,tachycardia,myocardial' >> solr-8.5.0/server/solr/covid19/conf/synonyms.txt;\n!echo 'pulmonary,respiratory' >> solr-8.5.0/server/solr/covid19/conf/synonyms.txt;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cat solr-8.5.0/server/solr/covid19/conf/synonyms.txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reload the covid19 core/collection because we added new synonyms. Need reload as it will affect index\n#Whenever new synonyms are added we need to reindex as synonyms are applied both on index and query analyzers\n!curl 'http://localhost:8983/solr/admin/cores?action=RELOAD&core=covid19'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Add custom field Type that wont tokenize phrases for fields like source etc\n!curl -X POST -H 'Content-type:application/json' --data-binary '{\"add-field-type\" : {\"name\":\"keywordText\",\"class\":\"solr.TextField\", \"positionIncrementGap\":\"100\", \"indexAnalyzer\" : {\"tokenizer\":{\"class\":\"solr.KeywordTokenizerFactory\" }, \"filters\":[{\"class\":\"solr.TrimFilterFactory\"},{\"class\":\"solr.StopFilterFactory\", \"ignoreCase\":true, \"words\":\"lang/stopwords_en.txt\"},{\"class\":\"solr.ManagedSynonymGraphFilterFactory\", \"managed\":\"english\" },{\"class\":\"solr.RemoveDuplicatesTokenFilterFactory\"},{\"class\":\"solr.FlattenGraphFilterFactory\"}]},\"queryAnalyzer\" : {\"tokenizer\":{\"class\":\"solr.KeywordTokenizerFactory\" },\"filters\":[{\"class\":\"solr.TrimFilterFactory\"},{\"class\":\"solr.StopFilterFactory\", \"ignoreCase\":true, \"words\":\"lang/stopwords_en.txt\"},{\"class\":\"solr.ManagedSynonymGraphFilterFactory\", \"managed\":\"english\" },{\"class\":\"solr.RemoveDuplicatesTokenFilterFactory\"}]}}}' http://localhost:8983/solr/covid19/schema","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create SOLR field definitions\n!curl -X POST -H 'Content-type:application/json' --data-binary '{\"add-field\": {\"name\":\"title\", \"type\":\"text_en_splitting_tight\", \"multiValued\":false, \"stored\":true, \"indexed\":true}}' http://localhost:8983/solr/covid19/schema;\n!curl -X POST -H 'Content-type:application/json' --data-binary '{\"add-field\": {\"name\":\"abstract\", \"type\":\"text_en_splitting_tight\", \"multiValued\":false, \"stored\":true, \"indexed\":true}}' http://localhost:8983/solr/covid19/schema;\n!curl -X POST -H 'Content-type:application/json' --data-binary '{\"add-field\": {\"name\":\"source\", \"type\":\"keywordText\", \"multiValued\":false, \"stored\":true, \"indexed\":true}}' http://localhost:8983/solr/covid19/schema;\n!curl -X POST -H 'Content-type:application/json' --data-binary '{\"add-field\": {\"name\":\"introduction\", \"type\":\"text_en_splitting_tight\", \"multiValued\":false, \"stored\":true, \"indexed\":true}}' http://localhost:8983/solr/covid19/schema;\n!curl -X POST -H 'Content-type:application/json' --data-binary '{\"add-field\": {\"name\":\"discussion\", \"type\":\"text_en_splitting_tight\", \"multiValued\":false, \"stored\":true, \"indexed\":true}}' http://localhost:8983/solr/covid19/schema;\n!curl -X POST -H 'Content-type:application/json' --data-binary '{\"add-field\": {\"name\":\"result\", \"type\":\"text_en_splitting_tight\", \"multiValued\":false, \"stored\":true, \"indexed\":true}}' http://localhost:8983/solr/covid19/schema;\n!curl -X POST -H 'Content-type:application/json' --data-binary '{\"add-field\": {\"name\":\"body\", \"type\":\"text_en_splitting_tight\", \"multiValued\":false, \"stored\":true, \"indexed\":true}}' http://localhost:8983/solr/covid19/schema;\n!curl -X POST -H 'Content-type:application/json' --data-binary '{\"add-field\": {\"name\":\"publish_time\", \"type\":\"pdate\", \"multiValued\":false, \"stored\":true, \"indexed\":true}}' http://localhost:8983/solr/covid19/schema;\n!curl -X POST -H 'Content-type:application/json' --data-binary '{\"add-field\": {\"name\":\"has_covid\", \"type\":\"boolean\", \"multiValued\":false, \"stored\":true, \"indexed\":true}}' http://localhost:8983/solr/covid19/schema;","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create Search Index"},{"metadata":{"trusted":true},"cell_type":"code","source":"solr = pysolr.Solr('http://localhost:8983/solr/covid19/', timeout=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generic_model = spacy.load('en_core_sci_md')\n\n#Load preprocessed CSV data\ncsv_path = '/kaggle/working/CORD-19.csv'\n\nif not path.exists(csv_path):\n    preprocess_data()\n    \ndf = pd.read_csv(csv_path, sep=',', header=0)\ndf.fillna('', inplace=True)\nprint('DF candidate_list size - ', df.shape)\n\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Index each pandas row as a document into SOLR search engine\n\ncovid_syns = ('SARSCoV2','SARS-CoV-2', '2019-nCoV','2019nCoV','COVID-19', 'COVID19','coronavirus', 'corona virus' 'novel coronavirus')\n\nlist_for_solr=[]\ncounter = 0\nfor index, row in df.iterrows():\n    id = row['paper_id']\n    title = row[\"title\"]\n    source = row[\"source\"]    \n    abstract = row[\"abstract\"]\n    introduction = row[\"introduction\"]\n    discussion = row[\"discussion\"]\n    result = row[\"result\"]  \n    body = row[\"body\"]  # Cocatenated text of all text fields abstract, introduction, discussion, result\n    \n    if((title and title.isspace()) and (abstract and abstract.isspace()) and (body and body.isspace())):\n        continue\n        \n    has_covid = 'false'\n    if any(words in body for words in covid_syns):\n        has_covid = 'true'\n    \n    solr_content = {}\n    solr_content['id'] = id\n    solr_content['title'] = title\n    solr_content['source'] = source\n    solr_content['abstract'] = abstract\n    solr_content['introduction'] = introduction\n    solr_content['discussion'] = discussion\n    solr_content['result'] = result    \n    solr_content['body'] = body\n    solr_content['has_covid'] = has_covid  \n        \n    list_for_solr.append(solr_content)\n    \n    if index % 100 == 0:\n        solr.add(list_for_solr)\n        list_for_solr = []\n        counter = counter + 100\n        print('Counter ', counter)\n        \n#Commit is very costly use it sparingly        \nsolr.commit()\nprint('Indexing Finished !')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_entities(models, text) :\n    entities = {}\n    \n    for nlp in models :\n        doc = nlp(text)\n        for ent in doc.ents:\n            entity = ent.text\n            if ent.label_ in entities :\n                if entities[ent.label_].count(ent.text) == 0:\n                    entities[ent.label_].append(ent.text)\n            else :\n                entities[ent.label_] = [ent.text]\n\n    return entities","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def initilize_nlp_models(model_names):\n    models = {}\n    for name in model_names:\n        models['name'] = spacy.load(name)\n    \n    print('Models Loaded')\n    return models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def search_task_answers(search_results) :\n    answers_list = list()\n    \n    for search_result in search_results:\n        doc_hl_dict = {}\n        \n        id = search_result.get('id', \"MISSING\")\n        title = search_result.get('title', \"MISSING\")\n        \n        doc_highlights = search_results.highlighting[id]\n        \n        doc_hl_dict['id'] = id\n        doc_hl_dict['title'] = title\n        \n        if len(doc_highlights) > 0:\n            display(HTML(f'<h4><i>{title}\\n</i></h4>'))\n\n        for doc_hl_field in doc_highlights:\n            hl_snippets = doc_highlights[doc_hl_field]\n        \n            if len(hl_snippets) > 0 :\n                answer_snippet = ''\n                \n                #print('ID : ', id, '\\nTITLE : ', title)\n                #print('\\t', doc_hl_field)\n                display(HTML(f'\\t<h5>{doc_hl_field}\\n</h5>\\n'))\n            \n                for index, snippet in enumerate(hl_snippets, start=1):\n                    answer_snippet = answer_snippet.strip() + \" \" + snippet.strip()\n                    \n                    display(HTML(f'<blockquote>{index}. {snippet.strip()}\\n</blockquote>'))\n                    #print('\\t\\t', index , '. ' , snippet.strip(), '\\n') \n                                  \n                doc_hl_dict[doc_hl_field] = answer_snippet.strip()\n                    \n        if len(doc_hl_dict) > 0:\n            answers_list.append(doc_hl_dict)\n        \n    return answers_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def search(query):\n    # Search for data\n    search_results = solr.search(query, **{\n    'fq':'has_covid:true',\n    'rows' : 20,\n    'qf':'title^50.0 abstract^40.0 introduction^30.0 discussion^20.0 result^50.0 body^10.0',\n    'pf':'title^60.0 abstract^50.0 introduction^40.0 discussion^30.0 result^60.0 body^20.0',\n    'hl': 'true',\n    'hl.bs.type': 'SENTENCE',\n    'hl.method' : 'unified',\n    'hl.snippets' : 5,\n    'hl.usePhraseHighlighter': 'true',\n    'hl.highlightMultiTerm' : 'true',\n    'hl.tag.pre':'',\n    'hl.tag.post':'',\n    'df':'body',\n    'hl.fl':'introduction,discussion,result'\n    })\n\n    num_docs_found = search_results.hits\n    num_search_results = len(search_results)\n    display(HTML(f'<h3 style=\"color:blue\">Top {num_search_results} search result(s) of {num_docs_found} total. \\n</h3>'))\n    \n#     print('Total Search Hits - ', num_docs_found)\n#     print(\"Got Top {0} result(s) of total {1}.\".format(len(search_results), num_docs_found))\n    \n    return num_docs_found, search_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def populate_labels(task_answers) :\n    field_list = ['introduction','discussion', 'result']\n\n    for doc_answer_dict in task_answers:\n        all_entities = set()\n        intro_entities = set()\n        discussion_entities = set()\n        result_entities = set() \n\n        for field_name, answer_text in doc_answer_dict.items():\n            if field_name in field_list:\n                ent_dict = extract_entities(chosen_models, answer_text)\n                ent_list = ent_dict['ENTITY']\n\n                all_entities.update(ent_list)\n                if field_name == 'introduction' :\n                    intro_entities.update(ent_list)\n                elif field_name == 'discussion' :\n                    discussion_entities.update(ent_list)\n                else :\n                    result_entities.update(ent_list)\n\n        # Now set up labels for entities\n        prior_newdata_entities = set()\n        prior_strong_entities = set()\n        prior_entities = set()\n        speculative_entities = set()\n        unknown_entities = set()    \n        novel_entities = set()\n\n        for a_ent in all_entities :\n            if a_ent in intro_entities and a_ent in result_entities and a_ent in discussion_entities : \n                prior_newdata_entities.add(a_ent)\n            elif a_ent in intro_entities and a_ent not in result_entities and a_ent in discussion_entities :\n                prior_strong_entities.add(a_ent)\n            elif a_ent in intro_entities and a_ent not in result_entities and a_ent not in discussion_entities :\n                prior_entities.add(a_ent)\n            elif a_ent not in intro_entities and a_ent in result_entities and a_ent not in discussion_entities : \n                unknown_entities.add(a_ent)\n            elif a_ent not in intro_entities and a_ent in result_entities and a_ent in discussion_entities :\n                novel_entities.add(a_ent)\n            else :\n                pass\n            \n        if(len(prior_newdata_entities) > 0) :\n            doc_answer_dict['prior-newdata'] = list(prior_newdata_entities)\n\n        if(len(prior_strong_entities) > 0) :\n            doc_answer_dict['prior-strong'] = list(prior_strong_entities)          \n\n        if(len(prior_entities) > 0) :\n            doc_answer_dict['prior'] = list(prior_entities)             \n\n        if(len(speculative_entities) > 0) :\n            doc_answer_dict['speculative'] = list(speculative_entities)\n\n        if(len(unknown_entities) > 0) :\n            doc_answer_dict['unknown'] = list(unknown_entities)           \n\n        if(len(novel_entities) > 0) :\n            doc_answer_dict['novel'] = list(novel_entities)           \n\n    return task_answers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### What do we know about COVID-19 risk factors?\n\nTask Details\n\nWhat do we know about COVID-19 risk factors? What have we learned from epidemiological studies?\nSpecifically, we want to know what the literature reports about:\n\n* Data on potential risks factors\n    1. Smoking, pre-existing pulmonary disease\n    2. Co-infections (determine whether co-existing respiratory/viral infections make the virus more transmissible or virulent) and other co-morbidities\n    3. Neonates and pregnant women\n    4. Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.\n* Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors\n* Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups\n* Susceptibility of populations\n* Public health mitigation measures that could be effective for control\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"tasks = ['Smoking and pre-existing pulmonary disease', \n         'Co-infections, co-morbidities and respiratory infections',\n         'Neonates and pregnant women',\n         'Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.',\n         'Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors',\n         'Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups',\n         'Susceptibility of populations',\n         'Public health mitigation measures that could be effective for control'\n        ]\n\nqueries = ['(\"Smoking COVID-19\"~10 OR \"pulmonary disease\"~10)', \n           '(\"Co-infections COVID-19\"~10 OR \"co-morbidities COVID-19\"~10 OR \"respiratory infections COVID-19\"~10)',\n           '(\"Neonates COVID-19\"~10 OR \"pregnant women COVID-19\"~10)',\n           'Socio-economic, behavioral factors and economic impact',\n           'Transmission dynamics, basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors',\n           'Severity of disease, risk of fatality among symptomatic hospitalized patients and high-risk patient groups',\n           'Susceptibility of populations',\n           'Public health mitigation measures that are effective for control'\n        ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Load NLP Models only once\n# model_names = ['en_ner_craft_md','en_ner_jnlpba_md', 'en_ner_bc5cdr_md','en_ner_bionlp13cg_md']\n# models_dict = initilize_nlp_models(model_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Map entities existence in intro-result-discussion respectively to label values\nlabel_def = {'111':'prior-newdata','101':'prior-strong','100':'prior','001':'speculative','010':'unknown', '011':'novel'}\nchosen_models = list()\nchosen_models.append(generic_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_def_list = list(label_def.values())\n\nfor task,query in zip(tasks, queries):\n    label_entities = []\n    display(HTML(f'<h3 style=\"color:red\">Task - {task} \\n</h3>'))\n    numDocsFound, search_results = search(query)\n    if numDocsFound > 0:\n        task_answers = search_task_answers(search_results)\n        task_answers = populate_labels(task_answers)\n        \n        for label in label_def_list:\n            for t_answer in task_answers:\n                if label in t_answer :\n                    label_entities = label_entities + t_answer[label]\n\n    cntr = Counter(label_entities)\n    freq_entities = cntr.most_common(20)\n\n    mc_df = pd.DataFrame(freq_entities, columns = ['Word', 'Count'])\n    mc_df.plot.bar(x='Word',y='Count')\n    plt.title(task)\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.show()\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def search_custom_query(query):\n    numDocsFound, search_results = search(query)\n    #display(search_results)\n    if numDocsFound > 0:\n        task_answers = search_task_answers(search_results)\n        task_answers = populate_labels(task_answers)\n        display(task_answers)\n\nsearchbar = widgets.interactive(search_custom_query, query='Pregnant women')\nsearchbar","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"task_answers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We will extract all the titles from the output and then assign them cosine scores. \n#### These title would then become our nodes and the cosine scores would define the distance between them"},{"metadata":{"trusted":true},"cell_type":"code","source":"def cosine_score(x):\n    d = []\n    for i in range(len(x)-1):\n        for j in range(i+1,len(x)):\n            doc1= generic_model(x[i])\n            doc2= generic_model(x[j])\n            d.append({\n\n                'Title1': x[i],\n                'Title2': x[j],\n                'Score': doc1.similarity(doc2)\n            }\n        )\n        \n    return d","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Extract the title list"},{"metadata":{"trusted":true},"cell_type":"code","source":"title_list = []\nfor title in range(len(task_answers)):\n    title_list.append(task_answers[title]['title'])\n\ntitle_dict_temp = cosine_score(title_list)\n\ntitle_df = pd.DataFrame(title_dict_temp)\n\ntitle_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Extract the word list along with their mappings to the titles and associated distance"},{"metadata":{"trusted":true},"cell_type":"code","source":"word_df = pd.DataFrame(task_answers)\n\nword_df.drop(['introduction','discussion','result'],axis=1,inplace=True)\n\nword_df.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_df_prior = word_df[['id','title','prior']]\nword_df_prior = word_df_prior.explode('prior')\nword_df_prior.dropna(subset = [\"prior\"], inplace=True)\nword_df_prior['Weight'] = 10\n\n\nword_df_prior_strong = word_df[['id','title','prior-strong']]\nword_df_prior_strong = word_df_prior_strong.explode('prior-strong')\nword_df_prior_strong.dropna(subset = [\"prior-strong\"], inplace=True)\nword_df_prior_strong['Weight'] = 5\n\n\n# word_df_prior_newdata = word_df[['id','title','prior_newdata']]\n# word_df_prior_newdata = word_df_prior_newdata.explode('prior_newdata')\n# word_df_prior_newdata.dropna(subset = [\"prior_newdata\"], inplace=True)\n# word_df_prior_newdata['Weight'] = 2\n\n\n# word_df_speculative = word_df[['id','title','speculative']]\n# word_df_speculative = word_df_speculative.explode('speculative')\n# word_df_speculative.dropna(subset = [\"speculative\"], inplace=True)\n# word_df_speculative['Weight'] = 15\n\n\n# word_df_unknown = word_df[['id','title','unknown']]\n# word_df_unknown = word_df_unknown.explode('unknown')\n# word_df_unknown.dropna(subset = [\"unknown\"], inplace=True)\n# word_df_unknown['Weight'] = 30\n\n\n\n# word_df_novel = word_df[['id','title','novel']]\n# word_df_novel = word_df_novel.explode('novel')\n# word_df_novel.dropna(subset = [\"novel\"], inplace=True)\n# word_df_novel['Weight'] = 25\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Collating all the words and titles together and associating different 'groups' to them so as to color code them"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take all the titles and categorize them into one group\ntitle_df_1 = title_df[['Title1']]\ntitle_df_1.drop_duplicates(inplace=True,keep='first')\ntitle_df_1['Color'] = 'group1'\ntitle_df_1.columns = ['Words','Color']\n\n# Combine all the words and categorize them into one group\n\nword_df_prior_1 = word_df_prior[['prior']]\nword_df_prior_1.drop_duplicates(inplace=True)\nword_df_prior_1['Color'] = 'group2'\nword_df_prior_1.columns = ['Words','Color']\n\n\nword_df_prior_strong_1 = word_df_prior_strong[['prior-strong']]\nword_df_prior_strong_1.drop_duplicates(inplace=True)\nword_df_prior_strong_1['Color'] = 'group2'\nword_df_prior_strong_1.columns = ['Words','Color']\n\n\n# Combining all the dataframe together in one place\n\nframes = [title_df_1,word_df_prior_1,word_df_prior_strong_1]\n\nmerged_df = pd.concat(frames)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_df.drop_duplicates(subset='Words',keep='first',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_df.set_index('Words',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Creating the graph and adding nodes and edges"},{"metadata":{"trusted":true},"cell_type":"code","source":"i = nx.Graph()\n\nfor row in title_df.iterrows():\n    i.add_edge(row[1]['Title1'], row[1]['Title1'], weight=row[1]['Score'])\n    \nfor row in word_df_prior.iterrows():\n    i.add_edge(row[1]['title'], row[1]['prior'], weight=row[1]['Weight'])\n    \nfor row in word_df_prior_strong.iterrows():\n    i.add_edge(row[1]['title'], row[1]['prior-strong'], weight=row[1]['Weight'])\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_df = merged_df.reindex(i.nodes())\n\nmerged_df['Color']=pd.Categorical(merged_df['Color'])\n\nmerged_df['Color'].cat.codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(22, 22))\ndegrees = nx.degree(i)\npos_node = nx.spring_layout(i,k=0.5)\nnx.draw_networkx(i,pos=pos_node,node_color=merged_df['Color'].cat.codes, cmap=plt.cm.Set2,node_size=[(degrees[v] + 1) * 100 for v in i.nodes()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}