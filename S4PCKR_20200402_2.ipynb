{"cells":[{"metadata":{"_uuid":"12e9dc8c-8513-4f4e-abfd-f656be4efc9a","_cell_guid":"1bca56d2-3a20-4c16-b977-d5d27c989cce","trusted":true},"cell_type":"code","source":"# %% [code]\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n# %% [code]\nimport numpy as np\nimport pandas as pd\nimport os\nimport json\nimport re\nfrom pandas import ExcelWriter\nfrom pandas import ExcelFile\n\n# %% [code]\ncustom_license_dir = '/kaggle/input/CORD-19-research-challenge/custom_license/custom_license/'\n\nnoncomm_use_subset_dir = '/kaggle/input/CORD-19-research-challenge/noncomm_use_subset/noncomm_use_subset/'\n\nbiorxiv_medrxiv_dir = '/kaggle/input/CORD-19-research-challenge/biorxiv_medrxiv/biorxiv_medrxiv/'\n\ncomm_use_subset = '/kaggle/input/CORD-19-research-challenge/comm_use_subset/comm_use_subset/'\n\n# %% [code]\n#List and print all files names in a directory\n\nfilenames_custom_license_dir = os.listdir(custom_license_dir)\n# print(\"custom_license_dir\")\n# print(\"Number of articles retrieved from noncomm_use_subset:\", len(filenames_custom_license_dir))\n# print(filenames_custom_license_dir)\n\n\nfilenames_noncomm_use_subset_dir = os.listdir(noncomm_use_subset_dir)\n# print(\"noncomm_use_subset_dir\")\n# print(\"Number of articles retrieved from noncomm_use_subset:\", len(filenames_noncomm_use_subset_dir))\n# print(filenames_noncomm_use_subset_dir)\n\n\nfilenames_biorxiv_medrxiv_dir = os.listdir(biorxiv_medrxiv_dir)\n# print(\"biorxiv_medrxiv_dir\")\n# print(\"Number of articles retrieved from biorxiv_medrxiv_dir:\", len(filenames_biorxiv_medrxiv_dir))\n# print(filenames_biorxiv_medrxiv_dir)\n\n\nfilenames_comm_use_subset_dir = os.listdir(comm_use_subset)\n# print(\"comm_use_subset\")\n# print(\"Number of articles retrieved from comm_use_subset:\", len(filenames_comm_use_subset_dir))\n# print(filenames_comm_use_subset_dir)\n\n# %% [code]\n#Load all files\n\nall_files_custom_license_dir = []\n\nfor filename in filenames_custom_license_dir:\n    filename = custom_license_dir + filename\n    file = json.load(open(filename, 'rb'))\n    all_files_custom_license_dir.append(file)\n\n\nall_files_noncomm_use_subset_dir = []\n\nfor filename in filenames_noncomm_use_subset_dir:\n    filename = noncomm_use_subset_dir + filename\n    file = json.load(open(filename, 'rb'))\n    all_files_noncomm_use_subset_dir.append(file)\n\n    \nall_files_biorxiv_medrxiv_dir = []\n\nfor filename in filenames_biorxiv_medrxiv_dir:\n    filename = biorxiv_medrxiv_dir + filename\n    file = json.load(open(filename, 'rb'))\n    all_files_biorxiv_medrxiv_dir.append(file)\n\n    \nall_files_comm_use_subset_dir = []\n\nfor filename in filenames_comm_use_subset_dir:\n    filename = comm_use_subset + filename\n    file = json.load(open(filename, 'rb'))\n    all_files_comm_use_subset_dir.append(file)\n\n# %% [code]\n#Print number of files loaded from each directory\n\nprint(len(all_files_custom_license_dir))\n\nprint(len(all_files_noncomm_use_subset_dir))\n\nprint(len(all_files_biorxiv_medrxiv_dir))\n\nprint(len(all_files_comm_use_subset_dir))\n\n# %% [code]\n# Combine in single dataframe\n\nall_files = [all_files_custom_license_dir, all_files_noncomm_use_subset_dir, all_files_biorxiv_medrxiv_dir, all_files_comm_use_subset_dir]\nprint(len(all_files))\n\n# %% [code]\nfile = all_files[0][0]\nprint(\"Dictionary keys:\", file.keys())\n\n# %% [code]\n# Read section data file\nsection_data = pd.read_excel('/kaggle/input/covidsectionheads/covid_section_heads.xlsx', sheet_name='Sheet1')\nsection_data.columns = ['section','frequency','category']\nsection_data.head(2)\n\n# %% [code]\n# List section categories of interest\nintroduction_categories = [\"Introduction\"]\ndiscussion_categories = [\"Discussion\", \"Results and Discussion\"]\nconclusion_categories = [\"Results\", \"Results and Discussion\"]\n\n\n# Create sub-lists\nintroduction_list_temp = section_data[section_data['category'].isin(introduction_categories)]\nintroduction_list = list(introduction_list_temp['section'])\n#print(introduction_list)\n\n\ndiscussion_list_temp = section_data[section_data['category'].isin(discussion_categories)]\ndiscussion_list = list(discussion_list_temp['section'])\n#print(discussion_list)\n\n\nconclusion_list_temp = section_data[section_data['category'].isin(conclusion_categories)]\nconclusion_list = list(conclusion_list_temp['section'])\n#print(conclusion_list)\n\n# %% [code]\n#Refine sub-lists (remove non-alphanumeric characters)\n\nintroduction_clean_list = [None]*len(introduction_list)\nfor value in range(len(introduction_list)):\n    value_tf = str(introduction_list[value]).replace(\" \",\"_\")\n    value_tf = re.sub(r'\\W+','', value_tf)\n    value_tf = value_tf.replace(\"_\", \" \")\n    value_tf = value_tf.strip()\n    introduction_clean_list[value] = value_tf\n\n\ndiscussion_clean_list = [None]*len(discussion_list)\nfor value in range(len(discussion_list)):\n    value_tf = str(discussion_list[value]).replace(\" \",\"_\")\n    value_tf = re.sub(r'\\W+','', value_tf)\n    value_tf = value_tf.replace(\"_\", \" \")\n    value_tf = value_tf.strip()\n    discussion_clean_list[value] = value_tf\n\n\nconclusion_clean_list = [None]*len(conclusion_list)\nfor value in range(len(conclusion_list)):\n    value_tf = str(conclusion_list[value]).replace(\" \",\"_\")\n    value_tf = re.sub(r'\\W+','', value_tf)\n    value_tf = value_tf.replace(\"_\", \" \")\n    value_tf = value_tf.strip()\n    conclusion_clean_list[value] = value_tf\n\n# %% [code]\n#Extract title\n\ndef findtitle_file(file):\n    metadatalength = len(file['metadata'])\n    if metadatalength > 0:\n        if \"title\" in file['metadata'].keys():\n            titlelength = len(file['metadata']['title'])\n            cleaneduptext = \"\"\n            cleaneduptext += file['metadata']['title']\n            return cleaneduptext\n\nfindtitle_file(all_files[0][30])\n\n# %% [code]\n#Extract abstract\n\ndef findabstract_file(file):\n    cleaneduptext  = \"\"\n    \n    abstractlength = len(file['abstract']) \n    for paragraph in range(abstractlength):\n        cleaneduptext += file['abstract'][paragraph]['text']      \n    return cleaneduptext\n\nfindabstract_file(all_files[1][0])\n\n# %% [code]\n#Extract introduction\n\ndef findintroduction_file(file, introduction_clean_list):\n    bodytextlength = len(file['body_text'])\n    cleaneduptext  = \"\"\n    \n#     print(cleaneduptext)\n    for paragraph in range(bodytextlength):\n#         print(\"Entered outer loop\")\n        searchspace = file['body_text'][paragraph]['section']\n#         print(searchspace)\n        for inputstring in introduction_clean_list:\n#             print(\"Entered inner loop\")\n#             print(inputstring)\n            if inputstring.upper() == searchspace.upper(): \n                cleaneduptext += file['body_text'][paragraph]['text']\n    \n    cleaneduptext = re.sub(r'\\([^)]*\\)', '', cleaneduptext)\n    cleaneduptext = re.sub(r'\\[[^)]*\\]', '', cleaneduptext)    \n    return cleaneduptext\n\nfindintroduction_file(all_files[1][2], introduction_clean_list)\n\n# %% [code]\n#Extract discussion\n\ndef finddiscussion_file(file, discussion_clean_list):\n    bodytextlength = len(file['body_text'])\n    cleaneduptext  = \"\"\n    \n    for paragraph in range(bodytextlength):\n        searchspace = file['body_text'][paragraph]['section']\n        for inputstring in discussion_clean_list:\n            if inputstring.upper() == searchspace.upper(): \n                cleaneduptext += file['body_text'][paragraph]['text']\n\n    cleaneduptext = re.sub(r'\\([^)]*\\)', '', cleaneduptext)\n    cleaneduptext = re.sub(r'\\[[^)]*\\]', '', cleaneduptext)\n    return cleaneduptext\n\nfinddiscussion_file(all_files[1][2], discussion_clean_list)\n\n# %% [code]\n#Extract conclusion\n\ndef findconclusion_file(file, conclusion_clean_list):\n    bodytextlength = len(file['body_text'])\n\n    cleaneduptext  = \"\"\n    \n    for paragraph in range(bodytextlength):\n        searchspace = file['body_text'][paragraph]['section']\n        for inputstring in conclusion_clean_list:\n            if inputstring.upper() == searchspace.upper(): \n                cleaneduptext += file['body_text'][paragraph]['text']\n\n    cleaneduptext = re.sub(r'\\([^)]*\\)', '', cleaneduptext)\n    cleaneduptext = re.sub(r'\\[[^)]*\\]', '', cleaneduptext)      \n    return cleaneduptext\n\nfindconclusion_file(all_files[1][2], conclusion_clean_list)\n\n# %% [code]\ncleaned_files = []\n\nfor directory in range(len(all_files)):\n    for file in all_files[directory]:\n        features = [\n            file['paper_id'],\n            findtitle_file(file),\n            findabstract_file(file),\n            findintroduction_file(file, introduction_clean_list),\n            finddiscussion_file(file, discussion_clean_list),\n            findconclusion_file(file, conclusion_clean_list),            \n            (findintroduction_file(file, introduction_clean_list)+finddiscussion_file(file, discussion_clean_list)+findconclusion_file(file, conclusion_clean_list))\n        ]\n        cleaned_files.append(features)\n\n# %% [code]\ncol_names = [\n    'paper_id', \n    'title',\n    'abstract',\n    'introduction',\n    'discussion',\n    'conclusion',\n    'body'\n]\n\nclean_df = []\nclean_df = pd.DataFrame(cleaned_files, columns=col_names)\nprint(len(clean_df))\nclean_df.head()\n\n# %% [code]\n# Identify if papers have sections\nsummary = []\n\ncount_no_introduction = 0\ncount_no_discussion = 0\ncount_no_conclusion = 0\ncount_no_body = 0\n\nfor row in range(len(clean_df)):\n    if len(clean_df['introduction'][row])>0:\n        has_introduction = 1\n    else:\n        has_introduction = 0\n        count_no_introduction += 1\n    if len(clean_df['discussion'][row])>0:\n        has_discussion = 1\n    else:\n        has_discussion = 0\n        count_no_discussion += 1\n    if len(clean_df['conclusion'][row])>0:\n        has_conclusion = 1\n    else:\n        has_conclusion = 0\n        count_no_conclusion += 1\n    if has_introduction == 0 and has_discussion == 0 and has_conclusion == 0:\n        has_body = 0\n        count_no_body += 1\n    else:\n        has_body = 1\n    features = [\n        clean_df['paper_id'][row],\n        has_introduction,\n        has_discussion,\n        has_conclusion,\n        has_body\n    ]\n    summary.append(features)\n\ncol_names = [\n    'paper_id', \n    'has_introduction',\n    'has_discussion',\n    'has_conclusion',\n    'has_body'\n]    \n   \nclean_summary = []\nclean_summary = pd.DataFrame(summary, columns=col_names)\n\n\nprint(\"No body in \"+str(count_no_body))\nprint(\"No introduction in \"+str(count_no_introduction))\nprint(\"No discussion in \"+str(count_no_discussion))\nprint(\"No conclusion in \"+str(count_no_conclusion))\n\n# %% [code]\nclean_summary[clean_summary['has_body']==0]['paper_id']\n\n# %% [code]\nlist(clean_summary[clean_summary['has_body']==0]['paper_id'])\n\n# %% [code]\n# delete_indices = clean_df[clean_df['paper_id'].isin(list(clean_summary[clean_summary['has_body']==0]['paper_id']))].index\n\n# clean_df.drop(delete_indices , inplace=True)\n# len(clean_df)\n\n# %% [code]\nmetadata = pd.read_csv('/kaggle/input/CORD-19-research-challenge/metadata.csv', usecols = ['sha', 'source_x'])\n\n# %% [code]\nmetadata.head()\n\n# %% [code]\nclean_df = clean_df.merge(metadata, how = 'left', left_on = 'paper_id', right_on = 'sha')\n\n# %% [code]\nclean_df.head()\n\n# %% [code]\nclean_df.drop(columns=['sha'], inplace = True)\n\n# %% [code]\nclean_df.rename(columns={\"source_x\": \"source\"}, inplace = True)\n\nclean_df.head()\n\n# %% [code]\nclean_df.to_csv('/kaggle/working/CORD-19.csv', index=False)\n\n# %% [code]\n!ls\n\n# %% [code]\n!java -version\n\n# %% [code]\n!wget -O solr-8.5.0.zip \"https://archive.apache.org/dist/lucene/solr/8.5.0/solr-8.5.0.zip\"\n\n# %% [code]\n!unzip solr-8.5.0.zip\n\n# %% [code]\n!ls\n\n# %% [code]\n!pip install pysolr\n!pip install kazoo #Needed for python zookeeper\n\n# %% [code]\n!solr-8.5.0/bin/solr start -force\n\n# %% [code]\n!solr-8.5.0/bin/solr create -c covid19 -s 1 -rf 1 -force\n\n# %% [code]\n# Using _default configset with data driven schema functionality. NOT RECOMMENDED for production use.\n!solr-8.5.0/bin/solr config -c covid19 -p 8983 -action set-user-property -property update.autoCreateFields -value false\n\n# %% [code]\n!curl -X POST -H 'Content-type:application/json' --data-binary '{\"add-field-type\" : {\"name\":\"keywordText\",\"class\":\"solr.TextField\", \"positionIncrementGap\":\"100\", \"indexAnalyzer\" : {\"tokenizer\":{\"class\":\"solr.KeywordTokenizerFactory\" }, \"filters\":[{\"class\":\"solr.TrimFilterFactory\"},{\"class\":\"solr.StopFilterFactory\", \"ignoreCase\":true, \"words\":\"lang/stopwords_en.txt\"},{\"class\":\"solr.ManagedSynonymGraphFilterFactory\", \"managed\":\"english\" },{\"class\":\"solr.RemoveDuplicatesTokenFilterFactory\"},{\"class\":\"solr.FlattenGraphFilterFactory\"}]},\"queryAnalyzer\" : {\"tokenizer\":{\"class\":\"solr.KeywordTokenizerFactory\" },\"filters\":[{\"class\":\"solr.TrimFilterFactory\"},{\"class\":\"solr.StopFilterFactory\", \"ignoreCase\":true, \"words\":\"lang/stopwords_en.txt\"},{\"class\":\"solr.ManagedSynonymGraphFilterFactory\", \"managed\":\"english\" },{\"class\":\"solr.RemoveDuplicatesTokenFilterFactory\"}]}}}' http://localhost:8983/solr/covid19/schema\n\n# %% [code]\n!curl -X POST -H 'Content-type:application/json' --data-binary '{\"add-field\": {\"name\":\"title\", \"type\":\"text_en_splitting\", \"multiValued\":false, \"stored\":true, \"indexed\":true}}' http://localhost:8983/solr/covid19/schema;\n!curl -X POST -H 'Content-type:application/json' --data-binary '{\"add-field\": {\"name\":\"abstract\", \"type\":\"text_en_splitting\", \"multiValued\":false, \"stored\":true, \"indexed\":true}}' http://localhost:8983/solr/covid19/schema;\n!curl -X POST -H 'Content-type:application/json' --data-binary '{\"add-field\": {\"name\":\"text_body\", \"type\":\"text_en_splitting\", \"multiValued\":false, \"stored\":true, \"indexed\":true}}' http://localhost:8983/solr/covid19/schema;\n!curl -X POST -H 'Content-type:application/json' --data-binary '{\"add-field\": {\"name\":\"source\", \"type\":\"keywordText\", \"multiValued\":false, \"stored\":true, \"indexed\":true}}' http://localhost:8983/solr/covid19/schema;\n\n# %% [code]\n\n\n# %% [code]\nimport pysolr\nsolr = pysolr.Solr('http://localhost:8983/solr/covid19/', timeout=10)\n\n# %% [code]\nsolr_content = {}\nsolr_content['id'] = 'id-9876'\nsolr_content['title'] = 'COVID-19 epidemic'\nsolr_content['source'] = 'Source-2'\nsolr_content['abstract'] = 'COVID-19 Epidemic causing tensions'\nsolr_content['text_body'] = 'Acute lung injury is a marked effect of the COVID-19 patients.'\nsolr.add([solr_content])\nsolr.commit()\n\n# %% [code]\n# Search for data\nresults = solr.search(\"\\\"acute lung injury\\\"\", **{\n    'hl': 'true',\n    'hl.fragsize': 100,\n    'hl.bs.type': 'SENTENCE',\n    'hl.method' : 'unified',\n    'hl.snippets' : 5,\n    'df':\"text_body\"\n})\n\nprint(\"Saw {0} result(s).\".format(len(results)))\n\nfor result in results:\n    print('TITLE : ', result['title'])\n    #display(HTML('<h1>' + result['title'][0]) + '</h1>')\n    hl_snippets = results.highlighting[result['id']]['text_body']\n    \n    for index, snippet in enumerate(hl_snippets, start=1):\n        print('\\t\\t', index, '. ', snippet, '\\n')\n    print('############################')","execution_count":0,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}